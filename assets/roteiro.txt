# ============================================================== Roteiro ================================================================

1. Introdução

Gancho: Como escalar o processamento de grandes volumes de dados de forma automatizada e eficiente?

Apresentação: "Fala pessoal, tudo bem? Eu me chamo Thiago Vinicius e hoje vou apresentar a vocês um projeto de Engenharia de Dados focado em Infraestrutura como
Código (IaC). O objetivo aqui é automatizar o provisionamento de um ecossistema de Big Data completo para processamentos de dados na nuvem com os serviços AWS."

O Problema: Configurar o Apache Spark, Hadoop, Hive e o Apache Flink, todos apartir do zero, leva muito tempo e exige bastante conhecimento do 
profissional que irá realizar essa tarefa, pois por serem ferramentas open-soucer, não existe instaladores, então o processo é todo manual. 
Já com o cluster EMR (Elastic MapReduce), com simples cliques, o profissional realiza todo esse processo em pouco tempo e pode integra-ló com outros serviços 
do mesmo ambiente AWS, como por exemplo: as Instancias EC2's, o Bucket S3 para armazenamento e serviços de IA como o SageMaker. 
Porém, configurar o cluster EMR e redes VPC's manualmente é demorado e propenso a erros. Sendo assim, utilizei o Terraform para automatizar esse processo.



2. Stack Tecnológica e Arquitetura (Mostrar o Diagrama de Arquitetura do Projeto)

A infraestrutura foi provisionada via Terraform, assegurando reprodutibilidade e governança. O ambiente AWS foi configurado em Multi-AZ para garantir alta 
disponibilidade e resiliência ao cluster de processamento.

Para testar esse poder de processamento na prática, vou executar o WordCount, um script clássico do Apache Flink para contagem de palavras. Vou utilizar 
um arquivo de texto armazenado no S3, que será processado dentro do sistema de arquivos distribuído (HDFS) no cluster EMR.
Ao final, a saída do Job será enviada de volta para este bucket S3. Além disso, configurei o envio automático dos logs para um bucket separado, garantindo 
a auditoria completa do pipeline de dados.

E para o Ambiente de Controle: Criei um container Docker que funciona como uma máquina cliente, isolando todas as ferramentas de CLI e chaves de acesso.



3. Estrutura do Código IaC

Dica: Mostre as pastas no VS Code (baseado nas imagens que você enviou).

Modularização: "Organizei o código em módulos para ser escalável:"

emr/: Contém as definições do cluster e das instâncias EC2. Também temos 2 camadas de segurança. A interna com o IAM Role que define as permissões de acesso 
do cluster EMR a outros serviços AWS. Como exemplo, o Amazon S3 é um serviço a parte do EMR e vai ser utilizado para armazenar os logs de processamento dos dados.
E para que isso seja possivel, tenho que permitir a integração entre ambos serviços. Tambem a camada externa com o grupo de segurança que funciona como um firewall definindo quem 
pode acessar a infraestrutura. Neste caso, vou permitir o acesso pela porta 22 que libera a conexão ssh atraves do container docker.

network/: Define a VPC e as subnets onde o cluster e outros serviços residem. Tambem habilito a camada de disponibilidade Multi-AZ. Essa opção não é obrigatoria,
entretanto, é uma boa prática que cobre um dos pilares do WAF (Well Architected Framework). Esse Framework é uma ferramenta que ajuda o cliente a entender
como projetar e operar sistemas de forma: confiavel, segura, eficiente e econômica na AWS. Pois já pensou se durante um grande processamento de dados, ocorresse
alguma interrupção imprevista na zona de disponibilidade onde está acontecendo esse trabalho? Independente qual fosse o motivo, como por exemplo: Crises naturais, alguma manutenção 
nos servidores ou algum outro motivo, todo esse processo seria perdido, em outras palavras, recursos financeiros disperdiçados, já que o EMR é um serviço pago desde o momento da 
criação do cluster. Já com a opção da camada multi-AZ habilitada, o pipeline de dados continuaria normalmente em outra região.

s3/: Criam os buckets para armazenar os logs, as saidas do job e o arquivo dados.txt que será processado.

ssh/: Será responsavel por gerar as chaves de acesso para que possamos realizar a conexão com cluster e gerencia-lo.



4. Realizar o projeto 


[Preparando o ambiente da máquina cliente com o DOCKER]


1-Criar a imagem do container tendo como base o arquivo de imagem Dockerfile.
    docker build -t dsa-terraform-image:projeto1 .


2-Criar o container Docker no Linux ou wsl 2 com o mapeamento de volume.
    docker run -dit --name dsa-projeto1 -v ./IaC:/iac dsa-terraform-image:projeto1 /bin/bash



[Preparando a automação com o TERRAFORM]


3-Configurar o AWS CLI para garantir a comunicação entre meu container e o ambiente nuvem AWS.
    aws configure


4-Iniciar o terraform no container docker no diretorio (iac/)
    terraform init

Ao executar o comando anterior (terraform init), ocorre um error de imagem (dubious ownership). Esse error significa que O Git (dentro do contêiner) percebeu 
que a pasta onde ele está tentando gravar pertence a um usuário diferente do usuário que está rodando o comando. Isso é super comum ao usar Docker 
com volumes montados no Windows/WSL (-v ./IaC:/iac). O Windows é o "dono" do arquivo, mas o Linux do contêiner é o "usuário". Para solucionar este
error, precisei dizer ao Git para "confiar" nessa pasta. Executei este comando dentro do meu terminal (no contêiner ou onde está rodando o Terraform):


git config --global --add safe.directory '*'


5-Cria o Plan e salva em disco
    terraform plan -var-file config.tfvars -out terraform.tfplan

6-Executa o apply com o arquivo de variáveis
    terraform apply -var-file config.tfvars

7-Ir até o ambiente AWS e conferir a criação do cluster EMR. Mostre as informações do cluster EMR por fim, mostre os steps (etapas)

8-Ir até o S3 e conferir os buckets

9-Ir no EC2 e conferir as instancias



[Preparando o ambiente com o HDFS - Hadoop]


10-Ir até o docker descktop e copiar a chave ssh gerada pela automação

11-Conexão via SSH ao master do cluster
    No container Docker com o comando tree mostre em detalhes a pasta generated e os seus conteudos.

12-Acesse o diretorio generated/ssh/ e mostre o nivel de privilegio do arquivo antes de executar a proxima etapa
    ls -la

13-Acesse o diretorio generated/ssh/ e ajuste o privilégio da chave privada com o comando abaixo
    chmod 400 deployer

14-Execute novamente o comando ls -la  para mostrar a alteração realizada no nivel de privilegio no arquivo deployer.
    ls -la

15-Conectar via SSH com o cluster EMR via ssh 
    Coloque o comando gerado pelo output "emr_main_connection_ssh" na etapa 8 anteriormente.

16-Mostrar tambem, que posso acessar o EMR através no ambiente AWS pelo navegador
    1. ls -la
    2. cd /
    3. ls -la
    4. cd /usr/
    5. ls -la
    6. cd lib/   (mostrar as bibliotecas existentes)
    7. ls -la
      
17-Explicar o funcionamento do sistema distribuido hadoop.
    O Hadoop trabalha com uma instancia master onde armazenamos os dados e executamos os comandos de processamentos com o Apache Flink, e com no minimo 
    2 instancias worker's que são as maquinas de processamento, se não, não seria possivel trabalhar de forma distribuida com somente uma maquina.
    Logo após, o hadoop divide os dados em blocos e distribui entre as maquinas de processamento. Depois de finalizar o processamento, agrupa todos os blocos
    processados e encaminha novamente para a maquina master com o resultado da tarefa.
    
18-Agora vou criar um diretorio como input de dados no hdfs (no container docker)
    1. hdfs dfs -ls / (Listamos todos os diretorios)
    2. hdfs dfs -ls /user
    3. hdfs dfs -ls /user/root (retorna vazia por que a pasta não existe)

19-Crie uma pasta como input no HDFS
    hdfs dfs -mkdir /user/root/input

20-Copie o arquivo dados.txt do bucket S3 para o sistema de arquivos distribuidos hadoop
    hdfs dfs -cp s3://dsa-p1-jobs-124645972365/job/dados.txt /user/root/input

21-Conferirmos se o arquivo foi armazenado com sucesso em no sistema distribuido
    hdfs dfs -ls /user/root/input


[Executar o exemplo de processamento com o Apache Flink]


22-Vamos até o diretorio da biblioteca do flink para mostrar os scrips de exemplos do flink
    1. cd /usr/lib/flink/
    2. ls
    3. cd examples/
    4. ls
    5. cd streaming/
    6. ls -la  

23-Voltamos para o diretório principal
    cd ~

24-Vamos contar o número de ocorrências de cada palavra no arquivo usando Apache Flink
    flink run -m yarn-cluster /usr/lib/flink/examples/streaming/WordCount.jar --input hdfs:///user/root/input/dados.txt --output hdfs:///user/root/saida/

25-Explicar como o flink funciona novamente.
    O Flink manda o comando para o cluster com o gerenciador de cluster, o yarn. Essa execução entra pela instancia master, procura pelos dados
    que já estão disponiveis nas maquinas worker's, busca os blocos de dados entrega para o processamento. O error thread-3 é um erro do proprio cluster
    durante execução que não afeta o processamento do job.

26-Copiar o arquivo de saida do job do HDFS para o sistema de arquivo local na minha maquina cliente no conteiner docker.
    1. hdfs dfs -ls /user/root/saida/
    2. hdfs dfs -ls /user/root/saida/"nova pasta criada do processamento"
    3. Explicar que a gente não ler arquivos direto do sistema hadoop, nos passamos para a maquina cliente, e ai sim, lermos os arquivos. 
    4. ls (na maquina cliente vai retornar tudo vazio)
    5. hdfs dfs -get /user/root/saida/"nova pasta criada do processamento"/"arquivo-saida-do-job"  
    6. ls -la (para verificar se o arquivo foi movido de forma correta para minha maquina cliente)
    7. cat "arquivo de saida-do-job" (Agora visualizamos o arquivo com o comando cat.)

27-Pronto, agora temos o nosso arquivo com o processamento de contagem de palavras. Esse processamento é bastante util para:
    
    1. Monitoramento de Redes Sociais e Sentimento, monitorando Hashtags ou termos específicos em tempo real.
   
    Utilidade: Identificar Trending Topics no exato momento em que surgem.
   
    Como exemplo: Uma empresa de marketing pode ajustar uma campanha em minutos se detectar um aumento súbito de termos negativos ou positivos associados 
    a uma marca.


    2. Análise de Logs e Segurança

    Sistemas de TI geram milhões de linhas de logs por segundo. Contar a ocorrência de "palavras" específicas (como códigos de erro 404, 500 ou a palavra 
    de falha de login) é crucial.

    Utilidade: Detecção de ataques de força bruta ou falhas críticas de infraestrutura.

    Como por exemplo: Se a contagem da palavra falha de autenticidade para um IP especifico exceder um limite (ex: 100 vezes em 1 minuto), o Flink pode 
    disparar um alerta imediato ou bloquear o tráfego.


    3. E-commerce e Pesquisa em Tempo Real

    Analisar o que os usuários estão digitando nas barras de busca dos sites.

    Utilidade: Ajustar o estoque ou sugerir produtos relacionados dinamicamente.

    Aplicação: Se a palavra "casaco de lã" começar a ter um pico de frequência em uma região específica (talvez devido a uma frente fria), o sistema de recomendação pode priorizar esses itens no topo da página.


[Preparar os passos (steps) necessários no Cluster EMR]

28-Agora vou mostrar como criar steps.
    Step (Passo) é, basicamente, uma unidade de trabalho ou uma tarefa específica que você envia para o cluster executar.
    Por exemplo, esse cluster que provisionei, está funcionando corretamente. Porém, eu estou precisando executar os comandos
    de processamento através da linha de comando da minha maquina cliente no docker.
    Já utilizando step, eu não preciso fazer isso. Eu posso criar uma lista de tarefas e entregar para o cluster faze-la
    de forma automatizada, sem eu precisar está vigiando ele. 

    E agora vou mostrar como isso funciona na prática. 

    Vou mandar o cluster copiar os dados para o hadoop, processar e armazenar em um arquivo e logo após, em um bucket S3


29- Os comandos abaixo devem ser executados no container Docker (no ambiente AWS)
OBS: Coloque o ID do seu cluster EMR

30- Explicar que eu posso executar esses comando tanto no docker como na plataforma AWS. Vou optar por executar atráves da
plataforma para mostrar que tambem posso executar tarefas tambem por la.

31 Explicar o comando.


Salvando em arquivo: Primeiro eu vou explicar o comando que adiciona o step que armazena o resultado de forma temporária no sistema
distribuido hadoop. O hadoop pessoal, funciona como uma memoria volatil, ou seja, quando falta energia todos os dados armazenados
somem. Porem, semelhando a memoria RAM, ele foi criado pra processamento, esse é motivo da sua alta eficacia em bigdata.

aws emr add-steps --cluster-id j-14RNI6Q1U71E3 \
--steps Type=CUSTOM_JAR,Name=Job1_P1,Jar=command-runner.jar,\
Args="flink","run","-m","yarn-cluster",\
"/usr/lib/flink/examples/streaming/WordCount.jar",\
"--input","hdfs:///user/root/input/dados.txt","--output","hdfs:///user/root/saidajob1/" \
--region us-east-2


32. Vá na pagina da AWS e mostre o Status do step Job1_P1.

33. Acesse o CLI para mostrar o resultado do processamento
    1. hdfs dfs -ls /user/root/saidajob1/
    2. hdfs dfs -ls /user/root/saidajob1/"diretorio-gerado"/"arquivo-gerado"

34. buscando o arquivo
    1. hdfs dfs -get /user/root/saidajob1/"diretorio-gerado"/"arquivo-gerado"
    2. ls ("arquivo-gerado" fora do hadoop)
    3. cat "arquivo-gerado"

35. Mostre os arquivos de logs dos steps
    1. controller (Este arquivo registra a interação entre o serviço AWS EMR e a execução do Step no cluster. Ele documenta o ciclo de vida do processo.)
    2. stderr (Este arquivo captura qualquer mensagem de erro ou exceção gerada pelo código ou pelo sistema operacional durante a execução.)
    3. stdout (Este arquivo armazena as saídas de texto normais geradas pelo programa enquanto ele é executado com sucesso.)

Salvando no Bucket S3: Armazena a saida do job de processamento de forma consistente.

aws emr add-steps --cluster-id j-14RNI6Q1U71E3 \
--steps Type=CUSTOM_JAR,Name=Job2_P1,Jar=command-runner.jar,\
Args="flink","run","-m","yarn-cluster",\
"/usr/lib/flink/examples/streaming/WordCount.jar",\
"--input","s3://dsa-p1-jobs-124645972365/job/dados.txt","--output","s3://dsa-p1-jobs-124645972365/" \
--region us-east-2

36. Vá na pagina da AWS e mostre o Status do step Job2_P1.

37. Ir no bucket s3 e mostrar os arquivos de job armazenados no bucket s3 p1-jobs.

38. Cria o Plan para o destroy e salva em disco
terraform plan -destroy -var-file config.tfvars -out terraform.tfplan

39. Executa o destroy
terraform apply terraform.tfplan